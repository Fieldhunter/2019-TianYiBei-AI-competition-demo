# 2019-TianYiBei-AI-competition-demo

[![](https://img.shields.io/badge/license-MIT-green)](https://github.com/Fieldhunter/2019-TianYiBei-AI-competition-demo/blob/master/LICENSE)
[![](https://img.shields.io/badge/author-Fieldhunter-blue)](https://github.com/Fieldhunter)
![](https://img.shields.io/badge/frame-keras-yellow)

2019“添翼杯”人工智能创新应用大赛-智慧环保赛道---代码敲不队demo

## 最终score

初赛Ａ榜：分数－－－94.02分；排名－－－23　　　　初赛Ｂ榜：分数－－－93.07分；排名－－－21

Ａ榜只是提供参考，B榜最终真实有效，前20名提供复赛资格。

## 说明

训练集等数据存储在data目录当中，但并没有上传。原data目录当中，mask目录下是遮挡物图片，Trainset目录下是原训练集图片，Test_A和Test_B是A,B榜的测试集。
各个模型的代码和history分别存放在model和history文件夹内。model文件内各模型的文件夹名格式为｛训练最后一轮cv上的AUC值｝。
history文件内各history的文件夹名格式为｛训练最后一轮cv上的AUC值_A榜测试集上的AUC值｝。各模型训练出来的.h5文件发布在release里面，命名规则与history一致。

## 数据处理

首先，根据赛题提供的训练集和测试集，发现训练集都是非常完整的图片，而测试集的图片都是非常不完整的，同时还有特定图案的遮挡物。为了让训练集更接近测试集，
首先先用Photoshop手工对测试集那些特定遮挡物抠出700张存储下来，用于做训练集的遮挡图案。

接下来的处理步骤每一次训练都会执行一遍（不保存到本地），这样使得训练集有一定的随机性，相当于在数量上做了数据增强。（初衷是可以将遍历过一遍训练集
的模型对另一批训练集进行再训练，但后来发现re-train的模型效果都会下降，故之后并没有采用re-train的模型）

在正式对每一张训练集的图片做处理时，首先从700张mask图案随机选取，然后对mask图案按顺序做随机平移和随机旋转，防止模型对仅有的700张mask图案过拟合。
然后将处理后的mask图案用掩膜的方式和训练集合在一起。再对合成后训练集图片按顺序做随机旋转，随机投影变换，随机平移，随机拉伸。用以上处理步骤来使训练集
图片更可能的接近测试集，来提高模型的泛化能力。

## 模型搭建

相应模型的具体细节可在model文件夹下查看相应源码。保存下来的模型只是效果较好的，中间调试了很多模型，效果不好的没有保存下来。

### 自定义模型

自定义模型分别是model目录下的84.84,84.96,85.1,87.64。总体思路就是按照卷积池化加全连接的顺序组合形式搭模型。

在训练调参的过程中发现，在卷积层上增加l2范数，以及卷积层和全连接层用RandomNormal初始化是有一定的效果的。同时发现，
在该比赛数据上用BatchNormalization模块，模型的LOSS值就根本下不去。这一点让我非常疑惑，按理来说BatchNormalization就是为了加速收敛以及轻微的正则化，
却在此失效，不是很理解其中的原因是什么。

在最后顺序结构调参无果时，自己尝试了一下自建残差模块（没有使用BatchNormalization），发现效果确实比之前的顺序结构要好很多。
但是，在此之后的用迁移学习的方法中发现，用在imagenet上预训练好的Resnet50却表现的非常差，cv集上的AUC值只有40%多一点，
这也让我更加怀疑是BatchNormalization模块的缘故。

### 迁移学习

迁移学习的模型分别是model目录下的93.73,95.47,95.67,95.94。用在imagenet上预训练好的VGG16,InceptionV3,InceptionResNetV2,Xception。
预训练的模型不带有全连接层，全连接层自己搭建并调参。其中Xception的效果是最好的，其次是InceptionResNetV2。

## 预测

用单模型去预测的分数，都在history文件夹内有写，单模型的分数并不是很高。之后采用融合模型的方法去预测。经过多次的尝试，用model目录下
93.73,95.47,95.67,95.94,87.64这５套模型去融合的效果是最好的。之后，尝试在融合过程中给５个模型分给别不同的权重，但发现都没有均分的效果来得好。
